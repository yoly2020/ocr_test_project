{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCR2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoly2020/ocr_test_project/blob/main/OCR2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc9WvG51soNf"
      },
      "source": [
        "###预处理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI9rfdTRLPjv"
      },
      "source": [
        "**IMPORTANT NOTE:**\n",
        "<br>_if you are using Google Colab rather than Jupyter notebook, please change all the **tf** in model settings to **tf.compat.v1**_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHILcoP-h7kB",
        "outputId": "19bb277d-7481-4908-df19-1dd69dbe9600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma9uFf5RsAlh"
      },
      "source": [
        "加载包"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA2bmRqRsFCp"
      },
      "source": [
        "# 1\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from itertools import product\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil\n",
        "import re\n",
        "import glob\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9EP5rTcpUVX"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/OCR Project/Logo Project/data/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLt8wuVSUU6b"
      },
      "source": [
        "# 2\n",
        "from scipy import ndimage\n",
        "from six.moves import cPickle as pickle\n",
        "import re\n",
        "import common "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKWisJnN1Ixt"
      },
      "source": [
        "# 3\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from six.moves import cPickle as pickle\n",
        "from six.moves import range\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGDo5mXZLIbI"
      },
      "source": [
        "# 4\n",
        "import tensorflow as tf\n",
        "import numpy as mp\n",
        "import os\n",
        "import sys\n",
        "from scipy import ndimage\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pkr7QBXLIbL"
      },
      "source": [
        "from __future__ import print_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkcmTntfm17R"
      },
      "source": [
        " 常数定义"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09k2Xzf8hvYB"
      },
      "source": [
        "# CNN input size constants\n",
        "CNN_IN_WIDTH=64\n",
        "CNN_IN_HEIGHT=32\n",
        "CNN_IN_CH=3\n",
        "\n",
        "# train valid test size constants\n",
        "TRAIN_SIZE=50000 # total train file: 163169\n",
        "VALID_SIZE=12500\n",
        "TEST_SIZE=12500 # total test file: 54425\n",
        "\n",
        "# data augmentation constants\n",
        "DATA_AUG_POS_SHIFT_MIN=-2\n",
        "DATA_AUG_POS_SHIFT_MAX=2\n",
        "DATA_AUG_SCALES=[0.9,1.1]\n",
        "DATA_AUG_ROT_MIN=-15\n",
        "DATA_AUG_ROT_MAX=15\n",
        "\n",
        "# 文件位置相关常量\n",
        "TRAIN_DIR='flickr_logos_27_dataset'\n",
        "TRAIN_IMAGE_DIR=os.path.join(TRAIN_DIR,'flickr_logos_27_dataset_images')\n",
        "CROPPED_AUG_IMAGE_DIR=os.path.join(TRAIN_DIR,'flickr_logos_27_dataset_cropped_augmented_images')\n",
        "ANNOT_FILE='flickr_logos_27_dataset_training_set_annotation.txt'\n",
        "NONE_IMAGE_DIR=os.path.join(TRAIN_DIR,'empty_dir')\n",
        "NUM_OF_NONE_IMAGES=1000\n",
        "\n",
        "# class constants\n",
        "CLASS_NAME = ['Adidas', 'Apple', 'BMW', 'Citroen', 'Cocacola', 'DHL', 'Fedex', 'Ferrari',\n",
        "    'Ford', 'Google', 'HP', 'Heineken', 'Intel', 'McDonalds', 'Mini', 'Nbc',\n",
        "    'Nike', 'Pepsi', 'Porsche', 'Puma', 'RedBull', 'Sprite', 'Starbucks',\n",
        "    'Texaco', 'Unicef', 'Vodafone', 'Yahoo']\n",
        "\n",
        "# image相关常量\n",
        "PIXEL_DEPTH=255.0\n",
        "\n",
        "# 用Pickle保存训练用的数据相关常量\n",
        "PICKLE_FILENAME='deep_logo.pickle'\n",
        "\n",
        "# tensorflow相关常量\n",
        "TF_TRAIN_DIR='flickr_logos_27_dataset'\n",
        "TF_MAX_STEPS=5001\n",
        "TF_IMAGE_WIDTH=64\n",
        "TF_IMAGE_HEIGHT=32\n",
        "TF_NUM_CLASSES=27\n",
        "TF_IMAGE_CHANNELS=3\n",
        "TF_LEARNING_RATE=0.0001\n",
        "TF_BATCH_SIZE=64\n",
        "TF_PATCH_SIZE=5 # 类似window size，每次cnnfilter作用于图片的一个patch大小的地方，而不是整个图片，对于kernel来说patch就是input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5f-9dkasZDI"
      },
      "source": [
        "###第一步，image augmentation and cropping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFM7oN8Pslud"
      },
      "source": [
        "定义所需函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z1hZ9j_pkMx"
      },
      "source": [
        "# 解析读入的有image标记的txt文件'flickr_logos_27_dataset_training_set_annotation.txt'\n",
        "\n",
        "def parse_annot(annot):\n",
        "  file_name=annot[0].decode('utf-8') # 解析utf8格式的string\n",
        "  class_name=annot[1].decode('utf-8')\n",
        "  train_subset_class=annot[2].decode('utf-8')\n",
        "  return file_name,class_name,train_subset_class\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGBnxppVqdgY"
      },
      "source": [
        "# 从标记文件中获取rectangle的信息\n",
        "\n",
        "def get_rect(annot):\n",
        "  rect=defaultdict(int)\n",
        "  x1,y1,x2,y2=rect_coord(annot[3:]) # top-left -> bottom-right\n",
        "  cx,cy,width,height=center_width_height(x1,y1,x2,y2)\n",
        "  rect['x1']=x1\n",
        "  rect['y1']=y1\n",
        "  rect['x2']=x2\n",
        "  rect['y2']=y2\n",
        "  rect['cx']=cx\n",
        "  rect['cy']=cy\n",
        "  rect['width']=width\n",
        "  rect['height']=height\n",
        "  \n",
        "  return rect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KsA9sK6tY7k"
      },
      "source": [
        "# 计算rectangle的中心坐标和高度、宽度\n",
        "\n",
        "def center_width_height(x1,y1,x2,y2):\n",
        "  cx=x1+(x2-x1)//2 # 向下取整的整除\n",
        "  cy=y1+(y2-y1)//2\n",
        "  width=(x2-x1)\n",
        "  height=(y2-y1)\n",
        "  return cx,cy,width,height"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9TpO-Ao3Az-"
      },
      "source": [
        "# 用pillow包来crop（取一部分）图像\n",
        "# 切换标记矩形的中心位置并获取变换过后的标记矩形中的这部分图像\n",
        "# resize以适应cnn输入大小\n",
        "\n",
        "def aug_pos(annot,img):\n",
        "  aug_pos_imgs=[] # 存储cropped images\n",
        "  aug_pos_suffixes=[] # 用于查看每次的shift值\n",
        "  rect=get_rect(annot)\n",
        "\n",
        "  for sx,sy in product(range(DATA_AUG_POS_SHIFT_MIN,DATA_AUG_POS_SHIFT_MAX),\n",
        "                       range(DATA_AUG_POS_SHIFT_MIN,DATA_AUG_POS_SHIFT_MAX)):\n",
        "    new_cx=rect['cx']+sx\n",
        "    new_cy=rect['cy']+sy\n",
        "\n",
        "    cropped_img=img.crop((new_cx-rect['width']//2,new_cy-rect['height']//2,\n",
        "                          new_cx+rect['width']//2,new_cy+rect['height']//2))\n",
        "    resized_img=cropped_img.resize((CNN_IN_WIDTH,CNN_IN_HEIGHT))\n",
        "\n",
        "    aug_pos_imgs.append(resized_img)\n",
        "    aug_pos_suffixes.append('p'+str(sx)+str(sy))\n",
        "    cropped_img.close()\n",
        "\n",
        "  return aug_pos_imgs,aug_pos_suffixes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGfErOwd7e3c"
      },
      "source": [
        "# 用pillow包来crop（取一部分）图像\n",
        "# scale标记矩形并获取变换过后的标记矩形中这部分图像\n",
        "# resize以适应cnn输入大小\n",
        "\n",
        "def aug_scale(annot,img):\n",
        "  aug_scale_imgs=[]\n",
        "  aug_scale_suffixes=[]\n",
        "  rect=get_rect(annot)\n",
        "  \n",
        "  for s in DATA_AUG_SCALES:\n",
        "    new_width=int(rect['width']*s)\n",
        "    new_height=int(rect['height']*s)\n",
        "\n",
        "    cropped_img=img.crop((rect['cx']-new_width//2,rect['cy']-new_height//2,\n",
        "                          rect['cx']+new_width//2,rect['cy']-new_height//2))\n",
        "    resized_img=cropped_img.resize((CNN_IN_WIDTH,CNN_IN_HEIGHT))\n",
        "\n",
        "    aug_scale_imgs.append(resized_img)\n",
        "    aug_scale_suffixes.append('s'+str(s))\n",
        "    cropped_img.close()\n",
        "\n",
        "  return aug_scale_imgs,aug_scale_suffixes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT4DwL5s9RAx"
      },
      "source": [
        "# 用pillow包来crop（取一部分）图像\n",
        "# 旋转图片并获取标记矩形中的这部分图像\n",
        "# resize以适应cnn输入大小\n",
        "\n",
        "def aug_rot(annot,img):\n",
        "  aug_rot_imgs=[]\n",
        "  aug_rot_suffixes=[]\n",
        "  rect=get_rect(annot)\n",
        "\n",
        "  for r in range(DATA_AUG_ROT_MIN,DATA_AUG_ROT_MAX):\n",
        "    rotated_img=img.rotate(r)\n",
        "    cropped_img=rotated_img.crop((rect['cx']-rect['width']//2,rect['cy']-rect['height']//2,\n",
        "                                  rect['cx']+rect['width']//2,rect['cy']+rect['height']//2))\n",
        "    resized_img=cropped_img.resize((CNN_IN_WIDTH,CNN_IN_HEIGHT))\n",
        "    \n",
        "    aug_rot_imgs.append(resized_img)\n",
        "    aug_rot_suffixes.append('r'+str(r))\n",
        "    rotated_img.close()\n",
        "    cropped_img.close()\n",
        "\n",
        "  return aug_rot_imgs,aug_rot_suffixes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1AHAerp_bXe"
      },
      "source": [
        "# crop图片以获取原图中标记矩形位置部分的图像\n",
        "# resize以适应cnn输入大小\n",
        "\n",
        "def crop_logos(annot,img):\n",
        "  x1,y1,x2,y2=rect_coord(annot[3:])\n",
        "\n",
        "  cropped_img=img.crop((x1,y1,x2,y2))\n",
        "  resized_img=cropped_img.resize((CNN_IN_WIDTH,CNN_IN_HEIGHT))\n",
        "  cropped_suffix='p00'\n",
        "  \n",
        "  return [resized_img],[cropped_suffix]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGEyMdSsDzni"
      },
      "source": [
        "def rect_coord(annot_part):\n",
        "  # annot_part表示annot文件中属于标记矩形的坐标的那部分信息\n",
        "  return list(map(int,annot_part)) # 是用map函数分别把annot文件中的坐标信息变成整数，再将结果放入list中"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz_-eTvuFgZr"
      },
      "source": [
        "# 判断图片是否需要skip掉，判断标准为标记矩形的长宽是否不大于0\n",
        "\n",
        "def is_skip(annot_part):\n",
        "  x1,y1,x2,y2=rect_coord(annot_part)\n",
        "  _,_,width,height=center_width_height(x1,y1,x2,y2)\n",
        "\n",
        "  if width<=0 or height <=0:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHjeLEEKGZ6_"
      },
      "source": [
        "# 处理过后的图片文件的保存命名格式\n",
        "\n",
        "def save_img(annot,cnt,*args):\n",
        "  # *args表示variable个数的argument\n",
        "  file_name,class_name,train_subset_class=parse_annot(annot)\n",
        "  dest_dir=os.path.join(CROPPED_AUG_IMAGE_DIR,class_name) # destination directory保存处理过的图片的目标文件夹\n",
        "  if not os.path.exists(dest_dir):\n",
        "    os.makedirs(dest_dir)\n",
        "  \n",
        "  for i,arg in enumerate(args):\n",
        "    for img,suffix in zip(arg[0],arg[1]):\n",
        "      save_file_name='_'.join([file_name.split('.')[0],class_name,train_subset_class,str(cnt),suffix]) + os.path.splitext(file_name)[1]\n",
        "      img.save(os.path.join(dest_dir,save_file_name)) # os.path.splitext()[1]获得root name i.e. '.jpg'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxHh9W9SMAYl"
      },
      "source": [
        "# 是用pillow包中的close函数来关闭图片的批量处理函数\n",
        "\n",
        "def close_img(*args):\n",
        "  for imgs in args:\n",
        "    for img in imgs:\n",
        "      img.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKbcBmidPJqJ"
      },
      "source": [
        "def crop_and_aug(annot_train):\n",
        "  cnt_per_file=defaultdict(int)\n",
        "\n",
        "  for annot in annot_train:\n",
        "    file_name,_,_=parse_annot(annot)\n",
        "    cnt_per_file[file_name]+=1 # 相同图片计数，因为同一张图片上可能有多个logo\n",
        "\n",
        "    if is_skip(annot[3:]):\n",
        "      print('Skip', file_name)\n",
        "      continue\n",
        "    \n",
        "    img=Image.open(os.path.join(TRAIN_IMAGE_DIR,file_name))\n",
        "\n",
        "    cropped_imgs,cropped_suffixes=crop_logos(annot,img)\n",
        "\n",
        "    shifted_imgs,shifted_suffixes=aug_pos(annot,img)\n",
        "\n",
        "    scaled_imgs,scaled_suffixes=aug_scale(annot,img)\n",
        "\n",
        "    rotated_imgs,rotated_suffixes=aug_rot(annot,img)\n",
        "\n",
        "    save_img(annot,cnt_per_file[file_name],[cropped_imgs,cropped_suffixes],[shifted_imgs,shifted_suffixes],\n",
        "             [scaled_imgs,scaled_suffixes],[rotated_imgs,rotated_suffixes])\n",
        "    \n",
        "    close_img([img],cropped_imgs,shifted_imgs,scaled_imgs,rotated_imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6gjC25uSQfG"
      },
      "source": [
        "def crop_none():\n",
        "    none_img_classes = [\n",
        "        cn.decode('utf-8')\n",
        "        for cn in np.loadtxt(\n",
        "            os.path.join(NONE_IMAGE_DIR, 'ClassName.txt'), dtype='a')\n",
        "    ]\n",
        "\n",
        "    dst_dir = os.path.join(CROPPED_AUG_IMAGE_DIR, 'None')\n",
        "    if not os.path.exists(dst_dir):\n",
        "        os.makedirs(dst_dir)\n",
        "\n",
        "    for none_class in none_img_classes:\n",
        "        none_dir = os.path.join(NONE_IMAGE_DIR, none_class[1:])\n",
        "        none_imgs = [\n",
        "            os.path.join(none_dir, img) for img in os.listdir(none_dir)\n",
        "            if re.search('\\.jpg', img)\n",
        "        ]\n",
        "        none_imgs = np.random.choice(none_imgs, 10)\n",
        "        for none_img in none_imgs:\n",
        "            im = Image.open(none_img)\n",
        "            if im.mode != \"RGB\":\n",
        "                im = im.convert(\"RGB\")\n",
        "            w, h = im.size\n",
        "            cw, ch = w // 2, h // 2\n",
        "            cropped_im = im.crop(\n",
        "                (cw - CNN_IN_WIDTH // 2, ch - CNN_IN_HEIGHT // 2,\n",
        "                 cw + CNN_IN_WIDTH // 2, ch + CNN_IN_HEIGHT // 2))\n",
        "            dst_fn = os.path.basename(none_img)\n",
        "            cropped_im.save(os.path.join(dst_dir, dst_fn))\n",
        "\n",
        "\n",
        "def crop_and_aug_with_none(annot_train, with_none=False):\n",
        "    # root directory to save processed images\n",
        "    if not os.path.exists(CROPPED_AUG_IMAGE_DIR):\n",
        "        os.makedirs(CROPPED_AUG_IMAGE_DIR)\n",
        "\n",
        "    # crop images and apply augmentation\n",
        "    crop_and_aug(annot_train)\n",
        "\n",
        "    # crop images of none class\n",
        "    if with_none:\n",
        "        crop_none()\n",
        "\n",
        "    # print results\n",
        "    org_imgs = [img for img in os.listdir(TRAIN_IMAGE_DIR)]\n",
        "    crop_and_aug_imgs = [\n",
        "        fname\n",
        "        for root, dirs, files in os.walk(CROPPED_AUG_IMAGE_DIR)\n",
        "        for fname in glob.glob(os.path.join(root, '*.jpg'))\n",
        "    ]\n",
        "    print('original: %d' % (len(org_imgs)))\n",
        "    print('cropped: %d' % (len(crop_and_aug_imgs)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FRgcUHjShuV"
      },
      "source": [
        "# 将aug过的图片文件夹中每个class文件夹中的img分成train和test部分并放入名为train和test的文件夹\n",
        "# 文件夹层级类似于：\n",
        "# --flickr_logos_27_dataset_cropped_augmented_images\n",
        "# ---Adidas\n",
        "# ----test\n",
        "# ----train\n",
        "\n",
        "def do_train_test_split():\n",
        "  class_names=[cls for cls in os.listdir(CROPPED_AUG_IMAGE_DIR)]\n",
        "\n",
        "  for class_name in class_names:\n",
        "    if os.path.exists(os.path.join(CROPPED_AUG_IMAGE_DIR,class_name,'train')):\n",
        "      continue\n",
        "    if os.path.exists(os.path.join(CROPPED_AUG_IMAGE_DIR,class_name,'test')):\n",
        "      continue\n",
        "    \n",
        "    imgs=[img for img in os.listdir(os.path.join(CROPPED_AUG_IMAGE_DIR,class_name))]\n",
        "\n",
        "    # train=0.75 test=0.25\n",
        "\n",
        "    train_imgs,test_imgs=train_test_split(imgs)\n",
        "\n",
        "    os.makedirs(os.path.join(CROPPED_AUG_IMAGE_DIR,class_name,'train'))\n",
        "    os.makedirs(os.path.join(CROPPED_AUG_IMAGE_DIR,class_name,'test'))\n",
        "\n",
        "    for img in train_imgs:\n",
        "      dest=os.path.join(CROPPED_AUG_IMAGE_DIR,class_name,'train')\n",
        "      src=os.path.join(CROPPED_AUG_IMAGE_DIR,class_name,img)\n",
        "\n",
        "      shutil.move(src,dest)\n",
        "\n",
        "    for img in test_imgs:\n",
        "      dest=os.path.join(CROPPED_AUG_IMAGE_DIR,class_name,'test')\n",
        "      src=os.path.join(CROPPED_AUG_IMAGE_DIR,class_name,img)\n",
        "\n",
        "      shutil.move(src,dest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi9HxOoVVBFg"
      },
      "source": [
        "执行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BwIO52rUbch",
        "outputId": "197194d6-1134-496d-e9dd-2b4544e815b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "annot_train=np.loadtxt(os.path.join(TRAIN_DIR,ANNOT_FILE),dtype='a')\n",
        "print('train_annotation: %d, %d' % (annot_train.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_annotation: 4536, 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aydk2jqkVWOw",
        "outputId": "4dac3753-11a9-4e2e-b4df-bfd744117499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "crop_and_aug_with_none(annot_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Skip 2662264721.jpg\n",
            "Skip 2662264721.jpg\n",
            "Skip 2662264721.jpg\n",
            "Skip 2662264721.jpg\n",
            "Skip 2662264721.jpg\n",
            "original: 1079\n",
            "cropped: 217488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKg-mzsdVdAY"
      },
      "source": [
        "do_train_test_split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz4jbhjvTxKh"
      },
      "source": [
        "###第二步，生成用于train、valid、test的数据"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU-GFLF71BGm"
      },
      "source": [
        "定义所需函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN7R9BwIT7Zd"
      },
      "source": [
        "# 从每个train或者test的文件夹中load logo图片并将处理过后的图片其放入dataset\n",
        "\n",
        "def load_logo(data_dir):\n",
        "  img_files=os.listdir(data_dir)\n",
        "  dataset=np.ndarray(shape=(len(img_files),CNN_IN_HEIGHT,CNN_IN_WIDTH,CNN_IN_CH),dtype=np.float32)\n",
        "  \n",
        "  print(data_dir)\n",
        "  \n",
        "  num_imgs=0\n",
        "\n",
        "  for img in img_files:\n",
        "    img_file=os.path.join(data_dir,img)\n",
        "    try:\n",
        "      img_data=(ndimage.imread(img_file).astype(float)-PIXEL_DEPTH/2)/PIXEL_DEPTH\n",
        "      if img_data.shape!=(CNN_IN_HEIGHT,CNN_IN_WIDTH,CNN_IN_CH):\n",
        "        raise Exception('Unexpected image shape: %s' % str(img_data.shape))\n",
        "      dataset[num_imgs,:,:]=img_data\n",
        "      num_imgs+=1\n",
        "    except IOError as e:\n",
        "      print('Could not read:', img_file,':',e,'-it\\'s ok, skipping.')\n",
        "  \n",
        "  dataset=dataset[0:num_imgs,:,:]\n",
        "  print('Full dataset tensor: ',dataset.shape)\n",
        "  print('Mean: ',np.mean(dataset))\n",
        "  print('Standard deviation: ',np.std(dataset))\n",
        "  # print('测试img_data到底是个啥：img_data=',img_data.shape,'\\n',img_data)\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrc7gjaKmKqk"
      },
      "source": [
        "# 把每个train、test文件夹中的数据以pickle格式保存在与train、test文件夹同层级的文件夹中\n",
        "# 返回存储着每个保存的pickle文件的全名的数组\n",
        "\n",
        "def maybe_pickle(data_dirs,force=False):\n",
        "  dataset_names=[]\n",
        "\n",
        "  for dir in data_dirs:\n",
        "    set_filename=dir+'.pickle'\n",
        "    dataset_names.append(set_filename)\n",
        "    \n",
        "    if os.path.exists(set_filename) and not force:\n",
        "      print('%s already present - Skipping pickling.' % set_filename)\n",
        "    else:\n",
        "      print('Pickling %s.' % set_filename)\n",
        "\n",
        "      dataset=load_logo(dir)\n",
        "\n",
        "      try:\n",
        "        with open(set_filename,'wb') as pickle_file:\n",
        "          pickle.dump(dataset,pickle_file,pickle.HIGHEST_PROTOCOL)\n",
        "      except Exception as e:\n",
        "        print('Unable to save data to',set_filename,':',e)\n",
        "  \n",
        "  return dataset_names\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV7HcoNvphHU"
      },
      "source": [
        "# 建立包含num_rows个（grayscale）图片的数组和其对应标签数组\n",
        "\n",
        "def make_arrays(num_rows,img_width,img_height,img_ch=1):\n",
        "  if num_rows:\n",
        "    dataset=np.ndarray((num_rows,img_height,img_width,img_ch),dtype=np.float32)\n",
        "    labels=np.ndarray(num_rows,dtype=np.int32)\n",
        "  else:\n",
        "    dataset,labels=None,None\n",
        "  return dataset,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Eg2vdDgh7Kc"
      },
      "source": [
        "# 建立包含train（，valid）和test的数据的数组：从许多个class文件夹中的pickle文件读取，并分配以及分别合并在一起\n",
        "\n",
        "def merge_datasets(pickle_files,train_size,valid_size=0):\n",
        "  num_classes=len(pickle_files)\n",
        "\n",
        "  valid_dataset,valid_labels=make_arrays(valid_size,CNN_IN_WIDTH,CNN_IN_HEIGHT,CNN_IN_CH)\n",
        "  train_dataset,train_labels=make_arrays(train_size,CNN_IN_WIDTH,CNN_IN_HEIGHT,CNN_IN_CH)\n",
        "  \n",
        "  valid_size_per_class=valid_size//num_classes\n",
        "  train_size_per_class=train_size//num_classes\n",
        "\n",
        "  start_valid,start_train=0,0\n",
        "  end_valid,end_train=valid_size_per_class,train_size_per_class\n",
        "  end_length=valid_size_per_class+train_size_per_class\n",
        "\n",
        "  for label,pickle_file in enumerate(pickle_files):\n",
        "    try:\n",
        "      with open(pickle_file,'rb') as file:\n",
        "        logo_set=pickle.load(file)\n",
        "        np.random.shuffle(logo_set) # 随机打乱每个class下的logo图片\n",
        "        if valid_dataset is not None:\n",
        "          # valid只有在传入train的时候从train里分配，传入test的时候不分配（虽然传入test的时候里面的相关变量还是叫train）\n",
        "          valid_logos=logo_set[:valid_size_per_class,:,:,:]\n",
        "\n",
        "          valid_dataset[start_valid:end_valid,:,:,:]=valid_logos\n",
        "          \n",
        "          valid_labels[start_valid:end_valid]=label # 都是同一个class啦\n",
        "\n",
        "          start_valid+=valid_size_per_class # 移动被填写数组的光标\n",
        "          end_valid+=valid_size_per_class\n",
        "        \n",
        "        train_logo=logo_set[valid_size_per_class:end_length,:,:,:] # train从logoset中valid的后面取\n",
        "\n",
        "        train_dataset[start_train:end_train,:,:,:]=train_logo\n",
        "\n",
        "        train_labels[start_train:end_train]=label\n",
        "\n",
        "        start_train+=train_size_per_class\n",
        "        end_train+=train_size_per_class\n",
        "\n",
        "    except Exception as e:\n",
        "      print('Unable to process data from', pickle_file,':',e)\n",
        "      raise\n",
        "  return valid_dataset,valid_labels,train_dataset,train_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv0oCHZjoOfS"
      },
      "source": [
        "# 打乱dataset和label数组，因为之前在merge_datasets里虽然用shuffle打乱了每个class底下的图片，但是还没有把全部class打乱(之前的洗牌在每个class里进行，现在的洗牌对所有牌进行)\n",
        "\n",
        "def randomize(dataset,labels):\n",
        "  permutation=np.random.permutation(labels.shape[0])\n",
        "  \n",
        "  shuffled_dataset=dataset[permutation,:,:]\n",
        "  shuffled_labels=labels[permutation]\n",
        "\n",
        "  return shuffled_dataset,shuffled_labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOxrTxG-u-rB"
      },
      "source": [
        "# 将融合后的train，valid，test的相关数据用pickle保存了\n",
        "\n",
        "def save_pickle(train_dataset,train_labels,valid_dataset,valid_labels,test_dataset,test_labels):\n",
        "  try:\n",
        "    file=open(PICKLE_FILENAME,'wb')\n",
        "    \n",
        "    save={\n",
        "        'train_dataset':train_dataset,\n",
        "        'train_labels':train_labels,\n",
        "        'valid_dataset':valid_dataset,\n",
        "        'valid_labels':valid_labels,\n",
        "        'test_dataset':test_dataset,\n",
        "        'test_labels':test_labels,\n",
        "    }\n",
        "\n",
        "    pickle.dump(save,file,pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    file.close()\n",
        "\n",
        "  except Exception as e:\n",
        "    print('Unable to save data to',PICKLE_FILENAME,':',e)\n",
        "    raise\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCHPWL9Jrrjk"
      },
      "source": [
        "执行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hv-p4xmru__"
      },
      "source": [
        "train_dirs=[os.path.join(CROPPED_AUG_IMAGE_DIR,class_name,'train') for class_name in CLASS_NAME]\n",
        "test_dirs=[os.path.join(CROPPED_AUG_IMAGE_DIR,class_name,'test') for class_name in CLASS_NAME]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmoWXoyBsPBV",
        "outputId": "3702ab01-3e41-451b-81d9-8776ce1ded59"
      },
      "source": [
        "train_datasets=maybe_pickle(train_dirs)\n",
        "test_datasets=maybe_pickle(test_dirs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Adidas/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Apple/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/BMW/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Citroen/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Cocacola/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/DHL/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Fedex/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Ferrari/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Ford/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Google/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/HP/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Heineken/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Intel/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/McDonalds/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Mini/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Nbc/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Nike/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Pepsi/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Porsche/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Puma/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/RedBull/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Sprite/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Starbucks/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Texaco/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Unicef/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Vodafone/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Yahoo/train.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Adidas/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Apple/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/BMW/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Citroen/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Cocacola/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/DHL/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Fedex/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Ferrari/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Ford/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Google/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/HP/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Heineken/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Intel/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/McDonalds/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Mini/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Nbc/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Nike/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Pepsi/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Porsche/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Puma/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/RedBull/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Sprite/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Starbucks/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Texaco/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Unicef/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Vodafone/test.pickle already present - Skipping pickling.\n",
            "flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Yahoo/test.pickle already present - Skipping pickling.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tszu5mS2sXmJ"
      },
      "source": [
        "valid_dataset,valid_labels,train_dataset,train_labels=merge_datasets(train_datasets,TRAIN_SIZE,VALID_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiO_SaLesoxI"
      },
      "source": [
        "_,_,test_dataset,test_labels=merge_datasets(test_datasets,TEST_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bPOqk6DtvuR"
      },
      "source": [
        "train_dataset,train_labels=randomize(train_dataset,train_labels)\n",
        "valid_dataset,valid_labels=randomize(valid_dataset,valid_labels)\n",
        "test_dataset,test_labels=randomize(test_dataset,test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRvI2i1pun1S",
        "outputId": "047c0914-4790-4de3-ac24-9f39357b5289"
      },
      "source": [
        "save_pickle(train_dataset,train_labels,valid_dataset,valid_labels,test_dataset,test_labels)\n",
        "statinfo=os.stat(PICKLE_FILENAME)\n",
        "print('Compressed pickle file size:',statinfo.st_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compressed pickle file size: 1843500511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxp0YHtR0IU2"
      },
      "source": [
        "###第三步，模型训练"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okr68Rwt1EXm"
      },
      "source": [
        "定义所需函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOC19hEv0P9a"
      },
      "source": [
        "# 计算预测的accuracy\n",
        "\n",
        "def accuracy(predictions,labels):\n",
        "  return (100*np.sum(np.argmax(predictions,1)==np.argmax(labels,1))/predictions.shape[0]) # 每行找出最大的下标进行比较,axis=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h90Nw07h5wSE"
      },
      "source": [
        "# 重编dataset和labels的形状\n",
        "\n",
        "def reformat(dataset,labels):\n",
        "  dataset=dataset.reshape((-1,TF_IMAGE_WIDTH,TF_IMAGE_HEIGHT,TF_IMAGE_CHANNELS)).astype(np.float32) # -1表示这个dimension的大小由numpy自己计算决定\n",
        "  labels=(np.arange(TF_NUM_CLASSES)==labels[:,None]).astype(np.float32) # 以1.0或0.0表示labels里的数是否在[0，27)内,形成一个有原labels.shape[0]行、27列，每行只有一个1.0的（二维）数组（类似于one-hot coding）\n",
        "\n",
        "  return dataset,labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOVEGcFXCuft"
      },
      "source": [
        "# 建立模型\n",
        "\n",
        "def my_model(data,filter_conv1,bias_conv1,filter_conv2,bias_conv2,filter_conv3,bias_conv3,filter_fc1,bias_fc1,filter_fc2,bias_fc2):\n",
        "  # Layer 1\n",
        "  h_conv1=tf.nn.relu(tf.nn.conv2d(data,filter_conv1,[1,1,1,1],padding='SAME')+bias_conv1)\n",
        "  print('filter_conv1:',filter_conv1.shape)\n",
        "  print('bias_conv1:',bias_conv1.shape)\n",
        "  print('bias.conv1:',bias_conv1)\n",
        "  print('h_conv1:',h_conv1.get_shape().as_list())\n",
        "\n",
        "  h_pool1=tf.nn.max_pool(h_conv1,ksize=[1,2,2,1],strides=[1,1,2,1],padding='SAME')\n",
        "\n",
        "  # Layer 2\n",
        "  h_conv2=tf.nn.relu(tf.nn.conv2d(h_pool1,filter_conv2,[1,1,1,1],padding='SAME')+bias_conv2)\n",
        "  h_pool2=tf.nn.max_pool(h_conv2,ksize=[1,1,2,1],strides=[1,1,2,1],padding='SAME')\n",
        "\n",
        "  # Layer 3\n",
        "  h_conv3=tf.nn.relu(tf.nn.conv2d(h_pool2,filter_conv3,[1,1,1,1],padding='SAME')+bias_conv3)\n",
        "  h_pool3=tf.nn.max_pool(h_conv3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
        "\n",
        "  # Fully Connected Layer\n",
        "  conv_layer_flat=tf.reshape(h_pool3,[-1,16*4*128])\n",
        "  h_fc1=tf.nn.relu(tf.matmul(conv_layer_flat,filter_fc1)+bias_fc1)\n",
        "\n",
        "  # Output Layer\n",
        "  out=tf.matmul(h_fc1,filter_fc2)+bias_fc2\n",
        "\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ4rO2lNLIb8"
      },
      "source": [
        "def my_model(data, w_conv1, b_conv1, w_conv2, b_conv2, w_conv3, b_conv3, w_fc1,\n",
        "          b_fc1, w_fc2, b_fc2):\n",
        "    # First layer\n",
        "    h_conv1 = tf.nn.relu(\n",
        "        tf.nn.conv2d(data, w_conv1, [1, 1, 1, 1], padding='SAME') + b_conv1)\n",
        "    h_pool1 = tf.nn.max_pool(\n",
        "        h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "    # Second layer\n",
        "    h_conv2 = tf.nn.relu(\n",
        "        tf.nn.conv2d(h_pool1, w_conv2, [1, 1, 1, 1], padding='SAME') + b_conv2)\n",
        "    h_pool2 = tf.nn.max_pool(\n",
        "        h_conv2, ksize=[1, 1, 2, 1], strides=[1, 1, 2, 1], padding='SAME')\n",
        "\n",
        "    # Third layer\n",
        "    h_conv3 = tf.nn.relu(\n",
        "        tf.nn.conv2d(h_pool2, w_conv3, [1, 1, 1, 1], padding='SAME') + b_conv3)\n",
        "    h_pool3 = tf.nn.max_pool(\n",
        "        h_conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "    # Fully connected layer\n",
        "    conv_layer_flat = tf.reshape(h_pool3, [-1, 16 * 4 * 128])\n",
        "    h_fc1 = tf.nn.relu(tf.matmul(conv_layer_flat, w_fc1) + b_fc1)\n",
        "\n",
        "    # Output layer\n",
        "    out = tf.matmul(h_fc1, w_fc2) + b_fc2\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAS5hzd3XsfS"
      },
      "source": [
        "# 从之前保存数据的deep_logo.pickle文件中读取数据\n",
        "\n",
        "def read_data():\n",
        "  with open(PICKLE_FILENAME,'rb') as file:\n",
        "    save=pickle.load(file)\n",
        "\n",
        "    train_dataset=save['train_dataset']\n",
        "    train_labels=save['train_labels']\n",
        "    valid_dataset=save['valid_dataset']\n",
        "    valid_labels=save['valid_labels']\n",
        "    test_dataset=save['test_dataset']\n",
        "    test_labels=save['test_labels']\n",
        "\n",
        "    del save\n",
        "\n",
        "    print('Training set',train_dataset.shape,train_labels.shape)\n",
        "    print('Valid set',valid_dataset.shape,valid_labels.shape)\n",
        "    print('Test set',test_dataset.shape,test_labels.shape)\n",
        "\n",
        "  return [train_dataset,valid_dataset,test_dataset],[train_labels,valid_labels,test_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6IOHBbVY-NO"
      },
      "source": [
        "执行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db5GaGnhZAHI"
      },
      "source": [
        "# if len(sys.argv) > 1:\n",
        "#     print(sys.argv)\n",
        "#     print(sys.argv[1])\n",
        "#     f=np.load(sys.argv[1])\n",
        "#     initial_weights=[\n",
        "#         f[n] for n in sorted(f.files,key=lambda s: int(s[4:]))]\n",
        "# else:\n",
        "#     initial_weights=None\n",
        "\n",
        "initial_weights=None\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "A2-f8o8fLIb-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDtts3knZ8Ps",
        "outputId": "425c9727-b4be-4ac4-9160-c07acdcf619e"
      },
      "source": [
        "dataset,labels=read_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (50000, 32, 64, 3) (50000,)\n",
            "Valid set (12500, 32, 64, 3) (12500,)\n",
            "Test set (12500, 32, 64, 3) (12500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPwE2gqQaae0",
        "outputId": "9680948d-8df3-4a15-db44-837b3d459056"
      },
      "source": [
        "train_dataset,train_labels=reformat(dataset[0],labels[0])\n",
        "valid_dataset,valid_labels=reformat(dataset[1],labels[1])\n",
        "test_dataset,test_labels=reformat(dataset[2],labels[2])\n",
        "\n",
        "print('Training set',train_dataset.shape,train_labels.shape)\n",
        "print('Valid set',valid_dataset.shape,valid_labels.shape)\n",
        "print('Test set',test_dataset.shape,test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (50000, 64, 32, 3) (50000, 27)\n",
            "Valid set (12500, 64, 32, 3) (12500, 27)\n",
            "Test set (12500, 64, 32, 3) (12500, 27)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f7Z4i5gatlz"
      },
      "source": [
        "# 关于training model的一些处理\n",
        "\n",
        "graph=tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "  # 1. Variables设定\n",
        "\n",
        "  # truncated_normal: 以mean为base做正态分布然后随机取值，大于2个标准差standard deviation（95%）的数据被抛弃（所以叫truncated）并重新选值\n",
        "  filter_conv1=tf.Variable(tf.truncated_normal([TF_PATCH_SIZE,TF_PATCH_SIZE,TF_IMAGE_CHANNELS,48],stddev=0.1)) # 那个1D array是the shape of the output tensor\n",
        "  bias_conv1=tf.Variable(tf.constant(0.1,shape=[48]))\n",
        "\n",
        "  filter_conv2=tf.Variable(tf.truncated_normal([TF_PATCH_SIZE,TF_PATCH_SIZE,48,64],stddev=0.1))\n",
        "  bias_conv2=tf.Variable(tf.constant(0.1,shape=[64]))\n",
        "\n",
        "  filter_conv3=tf.Variable(tf.truncated_normal([TF_PATCH_SIZE,TF_PATCH_SIZE,64,128],stddev=0.1))\n",
        "  bias_conv3=tf.Variable(tf.constant(0.1,shape=[128]))\n",
        "\n",
        "  filter_fc1=tf.Variable(tf.truncated_normal([16*4*128,2048],stddev=0.1))\n",
        "  bias_fc1=tf.Variable(tf.constant(0.1,shape=[2048]))\n",
        "\n",
        "  filter_fc2=tf.Variable(tf.truncated_normal([2048,TF_NUM_CLASSES]))\n",
        "  bias_fc2=tf.Variable(tf.constant(0.1,shape=[TF_NUM_CLASSES]))\n",
        "\n",
        "  # 2. Parameters设定\n",
        "\n",
        "  params=[filter_conv1,bias_conv1,filter_conv2,bias_conv2,filter_conv3,bias_conv3,filter_fc1,bias_fc1,filter_fc2,bias_fc2]\n",
        "\n",
        "  # 3. Initial Weights设定\n",
        "\n",
        "  if initial_weights is not None:\n",
        "    assert len(params) == len(initial_weights)\n",
        "    assign_ops=[w.assign(v) for w,v in zip(params,initial_weights)] # assign_operations creates a tf.Operation that you have to explicitly run to update the variable.\n",
        "\n",
        "  # 4. Input Data设定\n",
        "  \n",
        "  tf_train_dataset=tf.placeholder(tf.float32,shape=(TF_BATCH_SIZE,TF_IMAGE_WIDTH,TF_IMAGE_HEIGHT,TF_IMAGE_CHANNELS))\n",
        "  tf_train_labels=tf.placeholder(tf.float32,shape=(TF_BATCH_SIZE,TF_NUM_CLASSES))\n",
        "\n",
        "  tf_valid_dataset=tf.constant(valid_dataset) # constant tensor\n",
        "  \n",
        "  tf_test_dataset=tf.constant(test_dataset)\n",
        "\n",
        "  # 5. Training时相关计算量的设定\n",
        "  \n",
        "  logits=my_model(tf_train_dataset,filter_conv1,bias_conv1,filter_conv2,bias_conv2,\n",
        "                  filter_conv3,bias_conv3,filter_fc1,bias_fc1,filter_fc2,bias_fc2)\n",
        "\n",
        "  with tf.name_scope('loss'):\n",
        "    loss=tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=tf_train_labels))\n",
        "\n",
        "    tf.summary.scalar('loss',loss) # 将loss的值以标量的形式保存在tensorboard（tensorflow的可视化工具）里\n",
        "  \n",
        "  optimizer=tf.train.AdamOptimizer(TF_LEARNING_RATE).minimize(loss)\n",
        "\n",
        "  # 6. 对training、validation、testing data的predictions设定（注意logits是没有经过normalization的log probabilities\n",
        "  \n",
        "  train_prediction=tf.nn.softmax(logits)\n",
        "  valid_prediction=tf.nn.softmax(my_model(tf_valid_dataset,filter_conv1,bias_conv1,filter_conv2,bias_conv2,filter_conv3,bias_conv3,filter_fc1,bias_fc1,filter_fc2,bias_fc2))\n",
        "  test_prediction=tf.nn.softmax(my_model(tf_test_dataset,filter_conv1,bias_conv1,filter_conv2,bias_conv2,filter_conv3,bias_conv3,filter_fc1,bias_fc1,filter_fc2,bias_fc2))\n",
        "\n",
        "  # 7. 将所有summary信息merge并保存起来\n",
        "\n",
        "  merged=tf.summary.merge_all()\n",
        "  # merge_all 可以将所有summary全部保存到磁盘，以便tensorboard显示\n",
        "\n",
        "  train_writer=tf.summary.FileWriter(TF_TRAIN_DIR+'/train') # 指定一个文件加用来保存图\n",
        "\n",
        "  # 8. Add ops to save and restore all the variables.\n",
        "  \n",
        "  saver=tf.train.Saver()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iyz8c54mLIcC"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IP6OU9dWGD-",
        "outputId": "90fb11e5-014f-4a59-f660-ff3522d26f19"
      },
      "source": [
        "# Do Training\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  tf.global_variables_initializer().run() # 加了这步之后所有variables才真正hold the initial values you declare to them（相当于之前只是声明了但没有set）\n",
        "\n",
        "  if initial_weights is not None:\n",
        "    session.run(assign_ops)\n",
        "    print('Initialize by pre-learned values')\n",
        "\n",
        "  else:\n",
        "    print('Initialized')\n",
        "  \n",
        "  for step in range(TF_MAX_STEPS):\n",
        "    offset=(step*TF_BATCH_SIZE) % (train_labels.shape[0]-TF_BATCH_SIZE)\n",
        "\n",
        "#     if step <=5:\n",
        "#       print('offset:',offset)\n",
        "#       print('train_labels.shape[0]',train_labels.shape[0])\n",
        "    \n",
        "    batch_data=train_dataset[offset:(offset+TF_BATCH_SIZE),:,:,:]\n",
        "\n",
        "    batch_labels=train_labels[offset:(offset+TF_BATCH_SIZE),:]\n",
        "\n",
        "    feed_dict={\n",
        "        tf_train_dataset:batch_data,\n",
        "        tf_train_labels:batch_labels\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      _,l,predictions=session.run([optimizer,loss,train_prediction],feed_dict=feed_dict) # l: loss\n",
        "\n",
        "      if step%50 == 0:\n",
        "        summary,_=session.run([merged,optimizer],feed_dict=feed_dict)\n",
        "\n",
        "        train_writer.add_summary(summary,step)\n",
        "        print('Minibatch loss at step %d: %f' % (step,l))\n",
        "        print('Minibatch accuracy: %.1f%%' % accuracy(predictions,batch_labels))\n",
        "        print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(),valid_labels)) # 注意eval和run的区别\n",
        "    except KeyboardInterrupt:\n",
        "      last_weights=[p.eval() for p in params]\n",
        "      np.savez('weights.npz',*last_weights)\n",
        "\n",
        "      print('interrupted:',last_weights)\n",
        "  \n",
        "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(),test_labels))\n",
        "\n",
        "  # 保存训练好的模型\n",
        "  save_dir='models'\n",
        "  if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "  save_path=os.path.join(save_dir,\"deep_logo_model\")\n",
        "  saved=saver.save(session,save_path)\n",
        "  print(\"Model saved in file: %s\" % saved)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Minibatch loss at step 0: 58047.500000\n",
            "Minibatch accuracy: 6.2%\n",
            "Validation accuracy: 3.5%\n",
            "Minibatch loss at step 50: 8372.445312\n",
            "Minibatch accuracy: 20.3%\n",
            "Validation accuracy: 22.5%\n",
            "Minibatch loss at step 100: 3987.834229\n",
            "Minibatch accuracy: 39.1%\n",
            "Validation accuracy: 36.1%\n",
            "Minibatch loss at step 150: 3247.938721\n",
            "Minibatch accuracy: 42.2%\n",
            "Validation accuracy: 43.5%\n",
            "Minibatch loss at step 200: 2663.305176\n",
            "Minibatch accuracy: 40.6%\n",
            "Validation accuracy: 50.3%\n",
            "Minibatch loss at step 250: 2260.225098\n",
            "Minibatch accuracy: 62.5%\n",
            "Validation accuracy: 52.3%\n",
            "Minibatch loss at step 300: 2431.240723\n",
            "Minibatch accuracy: 48.4%\n",
            "Validation accuracy: 55.6%\n",
            "Minibatch loss at step 350: 1978.259644\n",
            "Minibatch accuracy: 60.9%\n",
            "Validation accuracy: 62.1%\n",
            "Minibatch loss at step 400: 2387.437256\n",
            "Minibatch accuracy: 53.1%\n",
            "Validation accuracy: 61.8%\n",
            "Minibatch loss at step 450: 2022.871582\n",
            "Minibatch accuracy: 62.5%\n",
            "Validation accuracy: 65.1%\n",
            "Minibatch loss at step 500: 1090.450439\n",
            "Minibatch accuracy: 68.8%\n",
            "Validation accuracy: 66.2%\n",
            "Minibatch loss at step 550: 1110.980469\n",
            "Minibatch accuracy: 67.2%\n",
            "Validation accuracy: 68.6%\n",
            "Minibatch loss at step 600: 980.302612\n",
            "Minibatch accuracy: 73.4%\n",
            "Validation accuracy: 68.7%\n",
            "Minibatch loss at step 650: 1069.111816\n",
            "Minibatch accuracy: 76.6%\n",
            "Validation accuracy: 72.4%\n",
            "Minibatch loss at step 700: 732.237427\n",
            "Minibatch accuracy: 73.4%\n",
            "Validation accuracy: 70.1%\n",
            "Minibatch loss at step 750: 921.781555\n",
            "Minibatch accuracy: 75.0%\n",
            "Validation accuracy: 73.0%\n",
            "Minibatch loss at step 800: 697.758423\n",
            "Minibatch accuracy: 78.1%\n",
            "Validation accuracy: 74.1%\n",
            "Minibatch loss at step 850: 722.766968\n",
            "Minibatch accuracy: 73.4%\n",
            "Validation accuracy: 75.6%\n",
            "Minibatch loss at step 900: 717.623413\n",
            "Minibatch accuracy: 82.8%\n",
            "Validation accuracy: 75.4%\n",
            "Minibatch loss at step 950: 982.432068\n",
            "Minibatch accuracy: 70.3%\n",
            "Validation accuracy: 76.2%\n",
            "Minibatch loss at step 1000: 854.513855\n",
            "Minibatch accuracy: 76.6%\n",
            "Validation accuracy: 77.2%\n",
            "Minibatch loss at step 1050: 439.840027\n",
            "Minibatch accuracy: 84.4%\n",
            "Validation accuracy: 76.6%\n",
            "Minibatch loss at step 1100: 273.517883\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 77.0%\n",
            "Minibatch loss at step 1150: 180.331955\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 79.9%\n",
            "Minibatch loss at step 1200: 555.075806\n",
            "Minibatch accuracy: 75.0%\n",
            "Validation accuracy: 80.2%\n",
            "Minibatch loss at step 1250: 289.191711\n",
            "Minibatch accuracy: 78.1%\n",
            "Validation accuracy: 79.5%\n",
            "Minibatch loss at step 1300: 188.168701\n",
            "Minibatch accuracy: 84.4%\n",
            "Validation accuracy: 80.5%\n",
            "Minibatch loss at step 1350: 285.355957\n",
            "Minibatch accuracy: 84.4%\n",
            "Validation accuracy: 78.9%\n",
            "Minibatch loss at step 1400: 371.922180\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 80.6%\n",
            "Minibatch loss at step 1450: 273.309357\n",
            "Minibatch accuracy: 84.4%\n",
            "Validation accuracy: 82.5%\n",
            "Minibatch loss at step 1500: 275.440887\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 81.4%\n",
            "Minibatch loss at step 1550: 304.249054\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 82.0%\n",
            "Minibatch loss at step 1600: 312.877441\n",
            "Minibatch accuracy: 81.2%\n",
            "Validation accuracy: 81.0%\n",
            "Minibatch loss at step 1650: 290.138245\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 82.8%\n",
            "Minibatch loss at step 1700: 383.643860\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 83.9%\n",
            "Minibatch loss at step 1750: 223.888321\n",
            "Minibatch accuracy: 78.1%\n",
            "Validation accuracy: 84.2%\n",
            "Minibatch loss at step 1800: 315.179016\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 83.7%\n",
            "Minibatch loss at step 1850: 286.931244\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 85.0%\n",
            "Minibatch loss at step 1900: 267.868378\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 84.8%\n",
            "Minibatch loss at step 1950: 362.945190\n",
            "Minibatch accuracy: 82.8%\n",
            "Validation accuracy: 84.0%\n",
            "Minibatch loss at step 2000: 77.348969\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 84.5%\n",
            "Minibatch loss at step 2050: 171.050125\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 84.5%\n",
            "Minibatch loss at step 2100: 352.449707\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 85.8%\n",
            "Minibatch loss at step 2150: 199.914322\n",
            "Minibatch accuracy: 90.6%\n",
            "Validation accuracy: 84.4%\n",
            "Minibatch loss at step 2200: 270.846741\n",
            "Minibatch accuracy: 85.9%\n",
            "Validation accuracy: 85.9%\n",
            "Minibatch loss at step 2250: 156.285583\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 86.7%\n",
            "Minibatch loss at step 2300: 25.418503\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 86.6%\n",
            "Minibatch loss at step 2350: 28.308163\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 86.3%\n",
            "Minibatch loss at step 2400: 114.254074\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 87.0%\n",
            "Minibatch loss at step 2450: 35.703613\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 85.0%\n",
            "Minibatch loss at step 2500: 77.862572\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 87.6%\n",
            "Minibatch loss at step 2550: 75.842674\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 87.5%\n",
            "Minibatch loss at step 2600: 47.743092\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 87.8%\n",
            "Minibatch loss at step 2650: 107.343254\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 88.0%\n",
            "Minibatch loss at step 2700: 30.223406\n",
            "Minibatch accuracy: 90.6%\n",
            "Validation accuracy: 87.8%\n",
            "Minibatch loss at step 2750: 86.358482\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 87.9%\n",
            "Minibatch loss at step 2800: 10.504413\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 85.3%\n",
            "Minibatch loss at step 2850: 7.578441\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 88.5%\n",
            "Minibatch loss at step 2900: 99.172310\n",
            "Minibatch accuracy: 87.5%\n",
            "Validation accuracy: 88.8%\n",
            "Minibatch loss at step 2950: 49.672356\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 88.3%\n",
            "Minibatch loss at step 3000: 94.947289\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 86.7%\n",
            "Minibatch loss at step 3050: 14.792808\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 89.1%\n",
            "Minibatch loss at step 3100: 51.038666\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 88.4%\n",
            "Minibatch loss at step 3150: 25.201042\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 88.6%\n",
            "Minibatch loss at step 3200: 29.542704\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 89.6%\n",
            "Minibatch loss at step 3250: 53.465023\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 87.6%\n",
            "Minibatch loss at step 3300: 10.546923\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 87.8%\n",
            "Minibatch loss at step 3350: 94.287468\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 87.2%\n",
            "Minibatch loss at step 3400: 8.835501\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 88.4%\n",
            "Minibatch loss at step 3450: 114.769234\n",
            "Minibatch accuracy: 89.1%\n",
            "Validation accuracy: 89.6%\n",
            "Minibatch loss at step 3500: 41.995350\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 90.0%\n",
            "Minibatch loss at step 3550: 55.076668\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 90.4%\n",
            "Minibatch loss at step 3600: 67.382385\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 89.2%\n",
            "Minibatch loss at step 3650: 68.271400\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 89.8%\n",
            "Minibatch loss at step 3700: 75.736305\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 89.5%\n",
            "Minibatch loss at step 3750: 33.885326\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 90.3%\n",
            "Minibatch loss at step 3800: 106.886063\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 90.5%\n",
            "Minibatch loss at step 3850: 127.200424\n",
            "Minibatch accuracy: 90.6%\n",
            "Validation accuracy: 90.5%\n",
            "Minibatch loss at step 3900: 138.644043\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 90.5%\n",
            "Minibatch loss at step 3950: 104.976990\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 89.1%\n",
            "Minibatch loss at step 4000: 29.529079\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 90.6%\n",
            "Minibatch loss at step 4050: 40.086327\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 90.3%\n",
            "Minibatch loss at step 4100: 36.426659\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 90.8%\n",
            "Minibatch loss at step 4150: 14.916524\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 90.0%\n",
            "Minibatch loss at step 4200: 35.909775\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 90.5%\n",
            "Minibatch loss at step 4250: 61.654907\n",
            "Minibatch accuracy: 93.8%\n",
            "Validation accuracy: 91.2%\n",
            "Minibatch loss at step 4300: 74.842918\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 88.9%\n",
            "Minibatch loss at step 4350: 84.503700\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 89.0%\n",
            "Minibatch loss at step 4400: 37.387581\n",
            "Minibatch accuracy: 93.8%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy: 90.2%\n",
            "Minibatch loss at step 4450: 4.347337\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 90.4%\n",
            "Minibatch loss at step 4500: 97.111618\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 88.8%\n",
            "Minibatch loss at step 4550: 36.005760\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 90.6%\n",
            "Minibatch loss at step 4600: 24.995491\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 90.8%\n",
            "Minibatch loss at step 4650: 41.060482\n",
            "Minibatch accuracy: 92.2%\n",
            "Validation accuracy: 91.4%\n",
            "Minibatch loss at step 4700: 32.821281\n",
            "Minibatch accuracy: 96.9%\n",
            "Validation accuracy: 90.9%\n",
            "Minibatch loss at step 4750: 21.457575\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 91.2%\n",
            "Minibatch loss at step 4800: 19.409849\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 91.0%\n",
            "Minibatch loss at step 4850: 8.985888\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 92.0%\n",
            "Minibatch loss at step 4900: 30.156622\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 91.2%\n",
            "Minibatch loss at step 4950: 20.354317\n",
            "Minibatch accuracy: 98.4%\n",
            "Validation accuracy: 91.8%\n",
            "Minibatch loss at step 5000: 28.383778\n",
            "Minibatch accuracy: 95.3%\n",
            "Validation accuracy: 90.7%\n",
            "Test accuracy: 91.0%\n",
            "Model saved in file: models/deep_logo_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txgSeKlSLIcE"
      },
      "source": [
        "###第四步，模型预测"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4i_2WGcLIcE"
      },
      "source": [
        "定义所需函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0e7WFPYLIcE"
      },
      "source": [
        "# 建立模型\n",
        "\n",
        "def my_model(data,filter_conv1,bias_conv1,filter_conv2,bias_conv2,filter_conv3,bias_conv3,filter_fc1,bias_fc1,filter_fc2,bias_fc2):\n",
        "  # Layer 1\n",
        "  h_conv1=tf.nn.relu(tf.nn.conv2d(data,filter_conv1,[1,1,1,1],padding='SAME')+bias_conv1)\n",
        "  print('filter_conv1:',filter_conv1.shape)\n",
        "  print('bias_conv1:',bias_conv1.shape)\n",
        "  print('bias.conv1:',bias_conv1)\n",
        "  print('h_conv1:',h_conv1.get_shape().as_list())\n",
        "\n",
        "  h_pool1=tf.nn.max_pool(h_conv1,ksize=[1,2,2,1],strides=[1,1,2,1],padding='SAME')\n",
        "\n",
        "  # Layer 2\n",
        "  h_conv2=tf.nn.relu(tf.nn.conv2d(h_pool1,filter_conv2,[1,1,1,1],padding='SAME')+bias_conv2)\n",
        "  h_pool2=tf.nn.max_pool(h_conv2,ksize=[1,1,2,1],strides=[1,1,2,1],padding='SAME')\n",
        "\n",
        "  # Layer 3\n",
        "  h_conv3=tf.nn.relu(tf.nn.conv2d(h_pool2,filter_conv3,[1,1,1,1],padding='SAME')+bias_conv3)\n",
        "  h_pool3=tf.nn.max_pool(h_conv3,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
        "\n",
        "  # Fully Connected Layer\n",
        "  conv_layer_flat=tf.reshape(h_pool3,[-1,16*4*128])\n",
        "  h_fc1=tf.nn.relu(tf.matmul(conv_layer_flat,filter_fc1)+bias_fc1)\n",
        "\n",
        "  # Output Layer\n",
        "  out=tf.matmul(h_fc1,filter_fc2)+bias_fc2\n",
        "\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwryBv8ALIcF"
      },
      "source": [
        "def load_initial_weights(fn):\n",
        "  f=np.load(fn)\n",
        "  initial_weights=[f[n] for n in sorted(f.files, key=lambda s: int(s[4:]))]\n",
        "  print(initial_weights)\n",
        "  return initial_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCQThT-6LIcG"
      },
      "source": [
        "执行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LmRjOnkLIcG",
        "outputId": "d0a64fb2-0cb7-4bb8-ef22-3fb6cefdb9af"
      },
      "source": [
        "# if len(sys.argv) > 1:\n",
        "#   test_image_fn=sys.argv[1]\n",
        "#   if not os.path.exists(test_image_fn):\n",
        "#     print(\"Not found:\", test_image_fn)\n",
        "#     sys.exit(-1)\n",
        "# else:\n",
        "#   # randomly select a image from test directory\n",
        "#   test_dirs=[os.path.join(CROPPED_AUG_IMAGE_DIR,class_name,'test') for class_name in CLASS_NAME]\n",
        "\n",
        "#   test_dir=np.random.choice(test_dirs)\n",
        "#   test_images_fn=[test_image for test_image in os.listdir(test_dir)]\n",
        "#   test_image_fn=np.random.choice(test_images_fn,1)[0]\n",
        "#   test_image_fn=os.path.join(test_dir,test_image_fn)\n",
        "\n",
        "test_dirs=[os.path.join(CROPPED_AUG_IMAGE_DIR,class_name,'test') for class_name in CLASS_NAME]\n",
        "\n",
        "test_dir=np.random.choice(test_dirs)\n",
        "test_images_fn=[test_image for test_image in os.listdir(test_dir)]\n",
        "test_image_fn=np.random.choice(test_images_fn,1)[0]\n",
        "test_image_fn=os.path.join(test_dir,test_image_fn)\n",
        "\n",
        "print('Test image:',test_image_fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test image: flickr_logos_27_dataset/flickr_logos_27_dataset_cropped_augmented_images/Unicef/test/4283515483_Unicef_2_8_p0-1.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDq3S5HLLIcH"
      },
      "source": [
        "test_image_org=(ndimage.imread(test_image_fn).astype(np.float32)-PIXEL_DEPTH / 2) / PIXEL_DEPTH\n",
        "\n",
        "test_image_org.resize((CNN_IN_HEIGHT,CNN_IN_WIDTH,CNN_IN_CH))\n",
        "\n",
        "test_image=test_image_org.reshape((1,CNN_IN_WIDTH,CNN_IN_HEIGHT,CNN_IN_CH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLhOaX4rLIcH"
      },
      "source": [
        "graph=tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "  filter_conv1=tf.Variable(tf.truncated_normal([TF_PATCH_SIZE,TF_PATCH_SIZE,TF_IMAGE_CHANNELS,48],stddev=0.1)) # 那个1D array是the shape of the output tensor\n",
        "  bias_conv1=tf.Variable(tf.constant(0.1,shape=[48]))\n",
        "\n",
        "  filter_conv2=tf.Variable(tf.truncated_normal([TF_PATCH_SIZE,TF_PATCH_SIZE,48,64],stddev=0.1))\n",
        "  bias_conv2=tf.Variable(tf.constant(0.1,shape=[64]))\n",
        "\n",
        "  filter_conv3=tf.Variable(tf.truncated_normal([TF_PATCH_SIZE,TF_PATCH_SIZE,64,128],stddev=0.1))\n",
        "  bias_conv3=tf.Variable(tf.constant(0.1,shape=[128]))\n",
        "\n",
        "  filter_fc1=tf.Variable(tf.truncated_normal([16*4*128,2048],stddev=0.1))\n",
        "  bias_fc1=tf.Variable(tf.constant(0.1,shape=[2048]))\n",
        "\n",
        "  filter_fc2=tf.Variable(tf.truncated_normal([2048,TF_NUM_CLASSES]))\n",
        "  bias_fc2=tf.Variable(tf.constant(0.1,shape=[TF_NUM_CLASSES]))\n",
        "\n",
        "  params=[filter_conv1,bias_conv1,filter_conv2,bias_conv2,filter_conv3,bias_conv3,filter_fc1,bias_fc1,filter_fc2,bias_fc2]\n",
        "\n",
        "  # restore weights\n",
        "  f=\"weights.npz\"\n",
        "\n",
        "  if os.path.exists(f):\n",
        "    initial_weights=load_initial_weights(f)\n",
        "  else:\n",
        "    initial_weights=None\n",
        "  \n",
        "  if initial_weights is not None:\n",
        "    assert len(initial_weights) == len(params)\n",
        "    assign_ops=[w.assign(v) for w,v in zip(params,initial_weights)]\n",
        "  \n",
        "  # placeholder for test image\n",
        "  tf_test_image=tf.constant(test_image)\n",
        "\n",
        "  logits=my_model(tf_test_image,filter_conv1,bias_conv1,filter_conv2,bias_conv2,filter_conv3,bias_conv3,filter_fc1,bias_fc1,filter_fc2,bias_fc2)\n",
        "\n",
        "  test_pred=tf.nn.softmax(logits)\n",
        "\n",
        "  saver=tf.train.Saver()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV5NB36TLIcI",
        "outputId": "1b1caf17-08ba-4ad3-900b-7cc4a07a3c72"
      },
      "source": [
        "with tf.Session(graph=graph) as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "\n",
        "  if initial_weights is not None:\n",
        "    session.run(assign_ops)\n",
        "    print('Initialized by pre-learned weights')\n",
        "  elif os.path.exists(\"models\"):\n",
        "    save_path=\"models/deep_logo_model\"\n",
        "    saver.restore(session,save_path)\n",
        "    print('Model restored')\n",
        "  else:\n",
        "    print('Initialized')\n",
        "  \n",
        "  pred=session.run([test_pred])\n",
        "\n",
        "  print(\"Class name:\",CLASS_NAME[np.argmax(pred)])\n",
        "  \n",
        "  print(\"Probability:\",np.max(pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from models/deep_logo_model\n",
            "Model restored\n",
            "Class name: Unicef\n",
            "Probability: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSQ_BkHzLIcJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}